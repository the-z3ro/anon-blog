---
title: "Can You Run Recursion on Ideas?"
date: 2026-01-18T15:37:51+05:30
draft: false
tags: ["ideas"]
categories: []
author: "Eshan"
showToc: false
TocOpen: false
description: "Notes from a few days of overthinking meaning, constraints, and AI"
cover:
  image: ""
  alt: ""
  caption: ""
---

I’ve only been thinking about this idea for the past two or three days, but it has completely hijacked my head. Not in a clean, organized way. More like it keeps popping up when I’m not trying to think about it, and every time it comes back, it looks slightly different. I don’t even know if I fully believe it yet, which is probably why I’m writing this down instead of just letting it sit in my head.

It started with that old philosophical thought experiment about the infinite library. The one where every possible book already exists. Every arrangement of letters. Every sentence that makes sense. And then an absurd amount of text that doesn’t. Somewhere in there is your biography, every possible version of your future, every scientific discovery, every incorrect idea, every correct one, all mixed together and buried under mountains of complete nonsense.

When I first heard about this idea years ago, my reaction was honestly pretty dismissive. It felt clever, but pointless. If almost everything in that library is garbage, then what does it matter that meaning exists somewhere inside it? An infinity with no structure doesn’t feel profound to me. It just feels unusable. Noise so large that it erases itself.

For a long time, that was where the idea ended for me. Something interesting to mention once and then move on from.

What changed recently, and I’m still not entirely sure why it clicked now, is thinking about this idea again in the context of AI. And I don’t mean AI as magic or intelligence or consciousness. I mean it in a very boring, mechanical sense.

Before, there was no way to filter that infinite library. Meaning was this messy, human thing. It lived in our heads. A machine could generate combinations forever, but it had no way to tell what was coherent and what wasn’t. It couldn’t tell whether something was connected, relevant, or even about anything at all. That was a hard limit.

That limit doesn’t feel as solid anymore.

AI doesn’t understand meaning the way humans do. I don’t think it does, at least. But it approximates understanding well enough to act as a filter. It can tell the difference between a paragraph that completely falls apart and one that actually follows a thought. It can stay on a topic. It can maintain consistency. It can notice when something contradicts itself. That might not be “understanding”, but it’s close enough to matter.

And that single shift started bothering me more than I expected.

If you take that infinite library and remove everything that has no meaning, you’re still left with something unimaginably large. But it’s not the same infinity anymore. It’s smaller. Still huge, but constrained. And constrained things can sometimes be explored, at least partially.

This is where I start doubting myself a bit, because this is also where the idea starts sounding too neat.

I began thinking about meaning not as a feeling, but as a constraint. Almost like a rule system. If meaning can be approximated, then it can be enforced. And if it can be enforced, then generation stops being random and starts being navigable.

That thought pulled me straight into recursion.

In computer science, recursion is not poetic. It’s blunt. You define a space. You define rules. You move step by step until you hit a stopping condition. Maze solving. Tree traversal. Backtracking. These are mechanical ideas. They don’t care how you feel about them. They just run.

So I asked myself a question that still feels slightly wrong to ask, what if we apply recursion not to numbers or paths, but to abstract ideas?

Not free creativity. Not storytelling in the usual sense. Controlled traversal.

Imagine starting with a small set of anchors. Something like a boy, twenty one years old, a forest, a traveller, introverted. These aren’t sentences. They’re just fixed points. They define what exists in the system.

Then imagine another list that doesn’t describe events, but pressure or direction. Words like accident, romantic, scared, bored, enlightened. These don’t say what happens. They say how meaning is allowed to move.

At this point, it already feels different from normal narrative thinking. It’s not “what happens next”. It’s “what kinds of connections are allowed to exist”.

Then there’s depth, which is the part I’m least confident about.

Instead of depth meaning time, depth could mean refinement. At one level, an entity can change physically. At another, it can gain a past. At another, its internal state or mood shifts. These levels aren’t random, at least not intentionally. They’re typed. Physical. Contextual. Psychological. I’m not convinced these are the right categories, but they’re a starting point.

The idea is that recursion doesn’t go deeper by adding more content, but by reducing ambiguity. Each level narrows the possibilities while making the state more specific.

Now imagine letting an AI explore all valid combinations under these rules. Fixed depth. Fixed word limit. No contradictions. Everything has to remain coherent. The AI isn’t inventing freely. It’s choosing valid transitions.

At some point, this stops looking like creativity and starts looking like a state space search. Abstract states as nodes. Semantic transitions as edges. Traversal under constraint.

If the traversal order is fixed, the constraints are fixed, and the evaluation rules are fixed, the output becomes surprisingly deterministic. Not identical in wording, but identical in structure and intent. That detail feels important, even if I’m not fully sure why yet.

This is also where I start feeling uneasy.

Because the boundaries matter too much.

The dimensions you choose decide what can exist at all. The connectors bias the entire space. The AI’s definition of coherence becomes an invisible authority. This isn’t objective truth. It’s structured approximation pretending to be complete.

What makes this more uncomfortable is realizing that human thinking works the same way. We also have unexamined dimensions. Cultural assumptions. Emotional biases. Language limits. We also traverse idea space recursively, just less explicitly and far less consistently.

I don’t think this kind of system could ever generate all possible ideas in any absolute sense. That would be dishonest to claim. But it might generate all possible ideas relative to a given schema. And that’s not that different from how formal systems work in mathematics. You pick axioms first, then see what follows.

I keep coming back to the question of novelty. If you exhaustively explore a constrained space, where does originality come from? Right now, the only answer I have is that novelty comes from changing the constraints themselves. New dimensions. New connectors. New evaluation rules. Creativity moves one level up.

Sometimes I wonder if we over-romanticize meaning. Maybe what feels deep is often just coherence sustained across many layers. Maybe meaning itself is a kind of constraint satisfaction problem that we don’t like describing that way.

I’m not confident about any of this. Parts of it feel solid. Other parts feel fragile. Some moments it feels like I’m circling something real. Other moments it feels like I’m forcing computational language onto philosophy because I want things to be structured.

I’m writing this not because I have answers, but because I don’t want to lose this line of thinking. This feels like the kind of idea that disappears if you don’t pin it down early, even if it turns out to be wrong.

If it leads nowhere, that’s fine. If it mutates into something else entirely, that’s also fine.

For now, this is just me trying to run recursion on my own thoughts and seeing where it breaks.
